#!/usr/bin/env python3
"""
DOCLAYOUT_DOCSTRUCTBENCH - Pythonæ ‡å‡†å®ç°
åŸºäºYOLOv8çš„æ–‡æ¡£å¸ƒå±€æ£€æµ‹æ ‡å‡†å®ç°

ä½œè€…: åŸºäºUltralytics YOLOv8æ ‡å‡†
æ—¥æœŸ: 2025-12-07
"""

import cv2
import numpy as np
import onnxruntime as ort
from typing import List, Tuple, Dict, Any
import argparse
import os
import json
from pathlib import Path

# DOCLAYOUT_DOCSTRUCTBENCH é…ç½®
DOCLAYOUT_CLASSES = [
    "title", "plain text", "abandon", "figure", "figure_caption",
    "table", "table_caption", "table_footnote", "isolate_formula", "formula_caption"
]

class DocLayoutAnalyzer:
    """DOCLAYOUT_DOCSTRUCTBENCH æ ‡å‡†å®ç°ç±»"""

    def __init__(self, model_path: str, conf_threshold: float = 0.2, iou_threshold: float = 0.4):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¸ƒå±€åˆ†æå™¨

        Args:
            model_path: ONNXæ¨¡å‹è·¯å¾„
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            iou_threshold: NMS IoUé˜ˆå€¼
        """
        self.model_path = model_path
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.input_size = (1024, 1024)  # DOCLAYOUT_DOCSTRUCTBENCHä½¿ç”¨1024x1024

        # åˆå§‹åŒ–ONNX Runtime
        self.session = None
        self.input_name = None
        self.output_names = None

        self._load_model()

    def _load_model(self):
        """åŠ è½½ONNXæ¨¡å‹"""
        try:
            # åˆ›å»ºONNX Runtimeä¼šè¯
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            self.session = ort.InferenceSession(self.model_path, providers=providers)

            # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
            self.input_name = self.session.get_inputs()[0].name
            self.output_names = [output.name for output in self.session.get_outputs()]

            print(f"âœ… Model loaded: {self.model_path}")
            print(f"ğŸ“‹ Input name: {self.input_name}")
            print(f"ğŸ“‹ Output names: {self.output_names}")

        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            raise

    def preprocess_image(self, image: np.ndarray) -> Tuple[np.ndarray, float, Tuple[int, int]]:
        """
        æ ‡å‡†YOLOv8é¢„å¤„ç† - Letterboxä¿æŒå®½é«˜æ¯”

        Args:
            image: è¾“å…¥å›¾åƒ (BGRæ ¼å¼)

        Returns:
            preprocessed_image: é¢„å¤„ç†åçš„å›¾åƒ
            gain: ç¼©æ”¾æ¯”ä¾‹
            padding: paddingå¤§å° (pad_w, pad_h)
        """
        original_shape = image.shape[:2]  # (h, w)

        # 1. è®¡ç®—gainï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
        gain = min(self.input_size[1] / original_shape[1], self.input_size[0] / original_shape[0])

        # 2. è®¡ç®—æ–°çš„å°ºå¯¸
        new_shape = (int(original_shape[1] * gain), int(original_shape[0] * gain))

        # 3. è®¡ç®—paddingï¼ˆä¸YOLOv8æ ‡å‡†ä¸€è‡´ï¼‰
        pad_w = round((self.input_size[1] - new_shape[0]) / 2 - 0.1)
        pad_h = round((self.input_size[0] - new_shape[1]) / 2 - 0.1)
        padding = (pad_w, pad_h)

        # 4. Resizeä¿æŒå®½é«˜æ¯”
        resized = cv2.resize(image, new_shape, interpolation=cv2.INTER_LINEAR)

        # 5. æ·»åŠ paddingï¼ˆä½¿ç”¨114,114,114ç°è‰²ï¼Œä¸YOLOæ ‡å‡†ä¸€è‡´ï¼‰
        padded = cv2.copyMakeBorder(resized, pad_h, pad_h, pad_w, pad_w,
                                   cv2.BORDER_CONSTANT, value=(114, 114, 114))

        # 6. BGR2RGB + å½’ä¸€åŒ–
        rgb = cv2.cvtColor(padded, cv2.COLOR_BGR2RGB)
        normalized = rgb.astype(np.float32) / 255.0

        # 7. HWC -> CHW
        chw = normalized.transpose(2, 0, 1)

        # 8. æ·»åŠ batchç»´åº¦
        batched = np.expand_dims(chw, axis=0)

        print(f"ğŸ”§ Preprocess: {original_shape[::-1]} -> {new_shape} -> {self.input_size}")
        print(f"ğŸ“ Gain: {gain:.3f}, Padding: {padding}")

        return batched, gain, padding

    def scale_boxes(self, boxes: np.ndarray, original_shape: Tuple[int, int],
                   gain: float, padding: Tuple[int, int]) -> np.ndarray:
        """
        æ ‡å‡†YOLOv8 scale_boxeså®ç°

        Args:
            boxes: æ£€æµ‹æ¡†åæ ‡ [N, 4] (x1, y1, x2, y2)
            original_shape: åŸå§‹å›¾åƒå°ºå¯¸ (h, w)
            gain: ç¼©æ”¾æ¯”ä¾‹
            padding: paddingå¤§å° (pad_w, pad_h)

        Returns:
            scaled_boxes: ç¼©æ”¾åçš„åæ ‡
        """
        pad_w, pad_h = padding

        # 1. å‡å»padding
        boxes[:, [0, 2]] -= pad_w  # x coordinates
        boxes[:, [1, 3]] -= pad_h  # y coordinates

        # 2. é™¤ä»¥gain
        boxes /= gain

        # 3. clip_boxes - é™åˆ¶åœ¨åŸå§‹å›¾åƒèŒƒå›´å†…
        boxes[:, [0, 2]] = np.clip(boxes[:, [0, 2]], 0, original_shape[1])  # width
        boxes[:, [1, 3]] = np.clip(boxes[:, [1, 3]], 0, original_shape[0])  # height

        return boxes

    def non_max_suppression(self, boxes: np.ndarray, scores: np.ndarray,
                          class_ids: np.ndarray) -> List[int]:
        """
        æ ‡å‡†NMSå®ç°

        Args:
            boxes: æ£€æµ‹æ¡†åæ ‡ [N, 4]
            scores: ç½®ä¿¡åº¦åˆ†æ•° [N]
            class_ids: ç±»åˆ«ID [N]

        Returns:
            keep_indices: ä¿ç•™çš„æ£€æµ‹æ¡†ç´¢å¼•
        """
        if len(boxes) == 0:
            return []

        # æŒ‰åˆ†æ•°é™åºæ’åº
        indices = np.argsort(scores)[::-1]

        keep = []
        while len(indices) > 0:
            # ä¿ç•™åˆ†æ•°æœ€é«˜çš„æ¡†
            current = indices[0]
            keep.append(current)

            if len(indices) == 1:
                break

            # è®¡ç®—å½“å‰æ¡†ä¸å…¶ä»–æ¡†çš„IoU
            current_box = boxes[current]
            other_boxes = boxes[indices[1:]]

            # è®¡ç®—IoU
            x1 = np.maximum(current_box[0], other_boxes[:, 0])
            y1 = np.maximum(current_box[1], other_boxes[:, 1])
            x2 = np.minimum(current_box[2], other_boxes[:, 2])
            y2 = np.minimum(current_box[3], other_boxes[:, 3])

            intersection = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)
            current_area = (current_box[2] - current_box[0]) * (current_box[3] - current_box[1])
            other_areas = (other_boxes[:, 2] - other_boxes[:, 0]) * (other_boxes[:, 3] - other_boxes[:, 1])

            union = current_area + other_areas - intersection
            iou = intersection / np.maximum(union, 1e-6)

            # ä¿ç•™IoUå°äºé˜ˆå€¼çš„æ¡†ï¼ˆåªå¯¹ä¸åŒç±»åˆ«è¿›è¡ŒNMSï¼‰
            same_class = class_ids[indices[1:]] == class_ids[current]
            indices = indices[1:][np.where(~same_class | (iou < self.iou_threshold))[0]]

        return keep

    def parse_yolo_output(self, output: np.ndarray, original_shape: Tuple[int, int],
                         gain: float, padding: Tuple[int, int]) -> List[Dict[str, Any]]:
        """
        è§£æYOLOv8è¾“å‡º

        Args:
            output: æ¨¡å‹è¾“å‡º [1, N, 6] (x1, y1, x2, y2, conf, class_id)
            original_shape: åŸå§‹å›¾åƒå°ºå¯¸ (h, w)
            gain: ç¼©æ”¾æ¯”ä¾‹
            padding: paddingå¤§å°

        Returns:
            detections: æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        # ç§»é™¤batchç»´åº¦
        output = output.squeeze(0)  # [N, 6]

        detections = []

        for i in range(len(output)):
            x1, y1, x2, y2, conf, class_id = output[i]

            # è¿‡æ»¤ä½ç½®ä¿¡åº¦å’Œæ— æ•ˆç±»åˆ«
            if conf < self.conf_threshold or class_id < 0 or class_id >= len(DOCLAYOUT_CLASSES):
                continue

            class_id = int(class_id)

            # ç¡®ä¿åæ ‡æœ‰æ•ˆ
            if x2 <= x1 or y2 <= y1:
                continue

            # åæ ‡å˜æ¢
            box = np.array([x1, y1, x2, y2])
            scaled_box = self.scale_boxes(box.reshape(1, -1), original_shape, gain, padding)[0]

            detection = {
                'bbox': scaled_box.astype(int).tolist(),  # [x1, y1, x2, y2]
                'confidence': float(conf),
                'class_id': class_id,
                'class_name': DOCLAYOUT_CLASSES[class_id]
            }

            detections.append(detection)

        return detections

    def detect(self, image: np.ndarray) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–‡æ¡£å¸ƒå±€æ£€æµ‹

        Args:
            image: è¾“å…¥å›¾åƒ (BGRæ ¼å¼)

        Returns:
            results: æ£€æµ‹ç»“æœ
        """
        original_shape = image.shape[:2]  # (h, w)

        # 1. é¢„å¤„ç†
        input_tensor, gain, padding = self.preprocess_image(image)

        # 2. æ¨ç†
        outputs = self.session.run(self.output_names, {self.input_name: input_tensor})
        output = outputs[0]  # [1, N, 6]

        print(f"ğŸ”® Model output shape: {output.shape}")

        # 3. è§£æè¾“å‡º
        detections = self.parse_yolo_output(output, original_shape, gain, padding)

        print(f"ğŸ“¦ Raw detections: {len(detections)}")

        # 4. NMS
        if detections:
            boxes = np.array([det['bbox'] for det in detections])
            scores = np.array([det['confidence'] for det in detections])
            class_ids = np.array([det['class_id'] for det in detections])

            keep_indices = self.non_max_suppression(boxes, scores, class_ids)
            detections = [detections[i] for i in keep_indices]

        print(f"ğŸ¯ Final detections after NMS: {len(detections)}")

        # 5. ç»Ÿè®¡ç»“æœ
        class_counts = {}
        for det in detections:
            class_name = det['class_name']
            class_counts[class_name] = class_counts.get(class_name, 0) + 1

        results = {
            'detections': detections,
            'class_counts': class_counts,
            'total_detections': len(detections),
            'original_shape': original_shape,
            'confidence_threshold': self.conf_threshold,
            'iou_threshold': self.iou_threshold
        }

        return results

    def visualize_results(self, image: np.ndarray, results: Dict[str, Any],
                         output_path: str = None) -> np.ndarray:
        """
        å¯è§†åŒ–æ£€æµ‹ç»“æœ

        Args:
            image: åŸå§‹å›¾åƒ
            results: æ£€æµ‹ç»“æœ
            output_path: è¾“å‡ºå›¾åƒè·¯å¾„

        Returns:
            annotated_image: æ ‡æ³¨åçš„å›¾åƒ
        """
        annotated = image.copy()

        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…é¢œè‰²
        colors = self._generate_colors(len(DOCLAYOUT_CLASSES))

        for detection in results['detections']:
            bbox = detection['bbox']
            class_name = detection['class_name']
            confidence = detection['confidence']
            class_id = detection['class_id']
            color = colors[class_id]

            # ç»˜åˆ¶æ£€æµ‹æ¡†
            cv2.rectangle(annotated, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)

            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{class_name} {confidence:.2f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]

            # æ ‡ç­¾èƒŒæ™¯
            cv2.rectangle(annotated,
                         (bbox[0], bbox[1] - label_size[1] - 10),
                         (bbox[0] + label_size[0], bbox[1]),
                         color, -1)

            # æ ‡ç­¾æ–‡å­—
            cv2.putText(annotated, label,
                       (bbox[0], bbox[1] - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        if output_path:
            cv2.imwrite(output_path, annotated)
            print(f"ğŸ’¾ Visualization saved to: {output_path}")

        return annotated

    def _generate_colors(self, num_classes: int) -> List[Tuple[int, int, int]]:
        """ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆä¸åŒçš„é¢œè‰²"""
        np.random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿é¢œè‰²ä¸€è‡´
        colors = []
        for i in range(num_classes):
            color = tuple(np.random.randint(0, 255, 3).tolist())
            colors.append(color)
        return colors

    def generate_markdown_report(self, results: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        markdown = "# DOCLAYOUT_DOCSTRUCTBENCH æ£€æµ‹æŠ¥å‘Š\n\n"

        # æ‘˜è¦ä¿¡æ¯
        markdown += "## ğŸ“Š æ£€æµ‹æ‘˜è¦\n\n"
        markdown += f"- **æ€»æ£€æµ‹æ•°é‡**: {results['total_detections']}\n"
        markdown += f"- **ç½®ä¿¡åº¦é˜ˆå€¼**: {results['confidence_threshold']}\n"
        markdown += f"- **IoUé˜ˆå€¼**: {results['iou_threshold']}\n"
        markdown += f"- **åŸå§‹å°ºå¯¸**: {results['original_shape'][0]} Ã— {results['original_shape'][1]}\n\n"

        # ç±»åˆ«ç»Ÿè®¡
        markdown += "## ğŸ“ˆ ç±»åˆ«ç»Ÿè®¡\n\n"
        for class_name, count in results['class_counts'].items():
            markdown += f"- **{class_name}**: {count}\n"
        markdown += "\n"

        # è¯¦ç»†æ£€æµ‹ç»“æœ
        markdown += "## ğŸ” è¯¦ç»†æ£€æµ‹ç»“æœ\n\n"
        for i, detection in enumerate(results['detections'], 1):
            bbox = detection['bbox']
            markdown += f"### {i}. {detection['class_name']}\n"
            markdown += f"- **ä½ç½®**: ({bbox[0]}, {bbox[1]}) â†’ ({bbox[2]}, {bbox[3]})\n"
            markdown += f"- **ç½®ä¿¡åº¦**: {detection['confidence']:.3f}\n"
            markdown += f"- **é¢ç§¯**: {(bbox[2]-bbox[0]) * (bbox[3]-bbox[1])} åƒç´ Â²\n\n"

        return markdown


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='DOCLAYOUT_DOCSTRUCTBENCH æ–‡æ¡£å¸ƒå±€æ£€æµ‹')
    parser.add_argument('--model', type=str, required=True, help='ONNXæ¨¡å‹è·¯å¾„')
    parser.add_argument('--image', type=str, required=True, help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--output', type=str, default='detection_result.jpg', help='è¾“å‡ºå›¾åƒè·¯å¾„')
    parser.add_argument('--report', type=str, default='detection_report.md', help='è¾“å‡ºæŠ¥å‘Šè·¯å¾„')
    parser.add_argument('--conf', type=float, default=0.2, help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--iou', type=float, default=0.4, help='NMS IoUé˜ˆå€¼')

    args = parser.parse_args()

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model):
        print(f"âŒ Model file not found: {args.model}")
        return

    if not os.path.exists(args.image):
        print(f"âŒ Image file not found: {args.image}")
        return

    # åˆ›å»ºåˆ†æå™¨
    print("ğŸš€ Initializing DOCLAYOUT_DOCSTRUCTBENCH analyzer...")
    analyzer = DocLayoutAnalyzer(args.model, args.conf, args.iou)

    # è¯»å–å›¾åƒ
    print("ğŸ“– Loading image...")
    image = cv2.imread(args.image)
    if image is None:
        print(f"âŒ Failed to load image: {args.image}")
        return

    # æ‰§è¡Œæ£€æµ‹
    print("ğŸ” Running detection...")
    results = analyzer.detect(image)

    # å¯è§†åŒ–ç»“æœ
    print("ğŸ¨ Visualizing results...")
    analyzer.visualize_results(image, results, args.output)

    # ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“ Generating report...")
    report = analyzer.generate_markdown_report(results)
    with open(args.report, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… Detection completed!")
    print(f"ğŸ“Š Total detections: {results['total_detections']}")
    print(f"ğŸ’¾ Result image: {args.output}")
    print(f"ğŸ“„ Report: {args.report}")


if __name__ == "__main__":
    main()